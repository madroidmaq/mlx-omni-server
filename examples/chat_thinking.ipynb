{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32b169b3",
   "metadata": {},
   "source": [
    "R# Chat Prompt Cached Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284cfb66",
   "metadata": {},
   "source": [
    "# Reasoning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d525d8b",
   "metadata": {},
   "source": [
    "## Qwen3-1.7B-4bit-DWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "401231f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e276a2cbd143d59616f1b74d0d9aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "<think>\n",
      "Okay, the user said \"hello\". I need to respond politely. Let me say \"Hello back!\" and offer help. Maybe add a friendly emoji to keep it warm. Keep it simple and friendly.\n",
      "</think>\n",
      "\n",
      "Hello back! How can I assist you today? üòä\n",
      "==========\n",
      "Prompt: 9 tokens, 285.441 tokens-per-sec\n",
      "Generation: 58 tokens, 203.997 tokens-per-sec\n",
      "Peak memory: 4.522 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/Qwen3-1.7B-4bit-DWQ\")\n",
    "\n",
    "prompt = \"hello\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27cd1834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user said \"hello\". I need to respond politely. Let me say \"Hello back!\" and offer help. Maybe add a friendly emoji to keep it warm. Keep it simple and friendly.\n",
      "Hello back! How can I assist you today? üòä\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_response(response):\n",
    "    # ÂåπÈÖç<think>Âíå</think>‰πãÈó¥ÁöÑÂÜÖÂÆπ\n",
    "    reasoning_regex = r'<think>([\\s\\S]*?)</think>'\n",
    "    reasoning_match = re.search(reasoning_regex, response)\n",
    "    reasoning_content = reasoning_match.group(1) if reasoning_match else ''\n",
    "    \n",
    "    # ÂåπÈÖç</think>‰πãÂêéÁöÑÊâÄÊúâÂÜÖÂÆπ\n",
    "    content_regex = r'</think>([\\s\\S]*)'\n",
    "    content_match = re.search(content_regex, response)\n",
    "    content = content_match.group(1) if content_match else ''\n",
    "    \n",
    "    return reasoning_content.strip(), content.strip()\n",
    "\n",
    "reasoning_content, content = parse_response(response)\n",
    "print(reasoning_content)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06a4c985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay, the user said \"hello\". I need to respond politely. Let me say \"Hello back!\" and offer help. Maybe add a friendly emoji to keep it warm. Keep it simple and friendly.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9489f5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85d2999d53f444185ee7e36316fc83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Hello! How can I assist you today?\n",
      "==========\n",
      "Prompt: 13 tokens, 330.749 tokens-per-sec\n",
      "Generation: 10 tokens, 195.830 tokens-per-sec\n",
      "Peak memory: 4.522 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/Qwen3-1.7B-4bit-DWQ\")\n",
    "\n",
    "prompt = \"hello\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False\n",
    ")\n",
    "\n",
    "response = generate(model, tokenizer, prompt=text, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc175d25",
   "metadata": {},
   "source": [
    "## Deepseek-R1-Distill-Qwen-1.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3323e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80962c0099bc4c48bca74b7f4d9dee4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>hello<ÔΩúAssistantÔΩú><think>\n",
      "\n",
      "==========\n",
      "Alright, the user said \"hello.\" That's a friendly greeting. I should respond in a warm and welcoming manner. Maybe say hello back and ask how I can assist them today. Keeping it open-ended encourages them to share what they need help with.\n",
      "</think>\n",
      "\n",
      "Hello! How can I assist you today?\n",
      "==========\n",
      "Prompt: 6 tokens, 165.337 tokens-per-sec\n",
      "Generation: 63 tokens, 105.433 tokens-per-sec\n",
      "Peak memory: 7.298 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "prompt = \"hello\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51365d79",
   "metadata": {},
   "source": [
    "# OpenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db5819fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Configure client to use local server\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:10240/v1\",  # Point to local server\n",
    "    api_key=\"not-needed\"  # API key is not required for local server\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3bafa8f",
   "metadata": {},
   "source": [
    "## v1/chat/completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "280fe5e6-0c7f-4554-93a9-0d30cce21f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: Hello! How can I assist you today?\n",
      "Reasoning: Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mlx-community/Qwen3-0.6B-4bit\",\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "content = completion.choices[0].message.content\n",
    "print(f\"Content: {content}\")\n",
    "print(f\"Reasoning: {completion.choices[0].message.reasoning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1d53327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta Content: Okay\n",
      "---\n",
      "delta Content: ,\n",
      "---\n",
      "delta Content:  I\n",
      "---\n",
      "delta Content:  need\n",
      "---\n",
      "delta Content:  to\n",
      "---\n",
      "delta Content:  work\n",
      "---\n",
      "delta Content:  on\n",
      "---\n",
      "delta Content:  writing\n",
      "---\n",
      "delta Content:  a\n",
      "---\n",
      "delta Content:  response\n",
      "---\n",
      "delta Content:  to\n",
      "---\n",
      "delta Content:  \"\n",
      "---\n",
      "delta Content: Hello\n",
      "---\n",
      "delta Content: !\"\n",
      "---\n",
      "delta Content:  from\n",
      "---\n",
      "delta Content:  the\n",
      "---\n",
      "delta Content:  user\n",
      "---\n",
      "delta Content: .\n",
      "---\n",
      "delta Content:  Let\n",
      "---\n",
      "delta Content:  me\n",
      "---\n",
      "delta Content:  think\n",
      "---\n",
      "delta Content: .\n",
      "---\n",
      "delta Content:  First\n",
      "---\n",
      "delta Content: ,\n",
      "---\n",
      "delta Content:  I\n",
      "---\n",
      "delta Content:  should\n",
      "---\n",
      "delta Content:  acknowledge\n",
      "---\n",
      "delta Content:  their\n",
      "---\n",
      "delta Content:  greeting\n",
      "---\n",
      "delta Content: .\n",
      "---\n",
      "delta Content:  Maybe\n",
      "---\n",
      "delta Content:  say\n",
      "---\n",
      "delta Content:  \"\n",
      "---\n",
      "delta Content: Hello\n",
      "---\n",
      "delta Content: !\"\n",
      "---\n",
      "delta Content:  and\n",
      "---\n",
      "delta Content:  greet\n",
      "---\n",
      "delta Content:  them\n",
      "---\n",
      "delta Content:  well\n",
      "---\n",
      "delta Content: .\n",
      "---\n",
      "delta Content:  It\n",
      "---\n",
      "delta Content: 's\n",
      "---\n",
      "delta Content:  a\n",
      "---\n",
      "delta Content:  basic\n",
      "---\n",
      "delta Content:  greeting\n",
      "---\n",
      "delta Content: ,\n",
      "---\n",
      "delta Content:  so\n",
      "---\n",
      "delta Content:  I\n",
      "---\n",
      "delta Content:  don\n",
      "---\n",
      "delta Content: 't\n",
      "---\n",
      "delta Content:  need\n",
      "---\n",
      "delta Content:  to\n",
      "---\n",
      "delta Content:  get\n",
      "---\n",
      "delta Content:  too\n",
      "---\n",
      "delta Content:  specific\n",
      "---\n",
      "delta Content: .\n",
      "---\n",
      "delta Content:  Also\n",
      "---\n",
      "delta Content: ,\n",
      "---\n",
      "delta Content:  using\n",
      "---\n",
      "delta Content:  appropriate\n",
      "---\n",
      "delta Content:  punctuation\n",
      "---\n",
      "delta Content: .\n",
      "---\n",
      "delta Content:  Let\n",
      "---\n",
      "delta Content:  me\n",
      "---\n",
      "delta Content:  check\n",
      "---\n",
      "delta Content:  for\n",
      "---\n",
      "delta Content:  any\n",
      "---\n",
      "delta Content:  possible\n",
      "---\n",
      "delta Content:  mistakes\n",
      "---\n",
      "delta Content: .\n",
      "---\n",
      "delta Content:  The\n",
      "---\n",
      "delta Content:  user\n",
      "---\n",
      "delta Content:  might\n",
      "---\n",
      "delta Content:  be\n",
      "---\n",
      "delta Content:  convers\n",
      "---\n",
      "delta Content: ing\n",
      "---\n",
      "delta Content:  a\n",
      "---\n",
      "delta Content:  bit\n",
      "---\n",
      "delta Content: ,\n",
      "---\n",
      "delta Content:  so\n",
      "---\n",
      "delta Content:  keeping\n",
      "---\n",
      "delta Content:  it\n",
      "---\n",
      "delta Content:  simple\n",
      "---\n",
      "delta Content:  and\n",
      "---\n",
      "delta Content:  natural\n",
      "---\n",
      "delta Content: .\n",
      "\n",
      "\n",
      "---\n",
      "delta Content: So\n",
      "---\n",
      "delta Content: ,\n",
      "---\n",
      "delta Content:  starting\n",
      "---\n",
      "delta Content:  with\n",
      "---\n",
      "delta Content:  \"\n",
      "---\n",
      "delta Content: Hi\n",
      "---\n",
      "delta Content: !\",\n",
      "---\n",
      "delta Content:  that\n",
      "---\n",
      "delta Content: 's\n",
      "---\n",
      "delta Content:  the\n",
      "---\n",
      "delta Content:  most\n",
      "---\n",
      "delta Content:  natural\n",
      "---\n",
      "delta Content:  way\n",
      "---\n",
      "delta Content: .\n",
      "---\n",
      "delta Content:  Then\n",
      "---\n",
      "delta Content: ,\n",
      "---\n",
      "delta Content:  maybe\n",
      "---\n",
      "delta Content:  a\n",
      "---\n",
      "delta Content:  bit\n",
      "---\n",
      "delta Content:  of\n",
      "---\n",
      "delta Content:  a\n",
      "---\n",
      "delta Content:  friendly\n",
      "---\n",
      "delta Content:  greeting\n",
      "---\n",
      "delta Content:  like\n",
      "---\n",
      "delta Content:  \"\n",
      "---\n",
      "delta Content: You\n",
      "---\n",
      "delta Content: 're\n",
      "---\n",
      "delta Content:  doing\n",
      "---\n",
      "delta Content:  great\n",
      "---\n",
      "delta Content: !\"\n",
      "---\n",
      "delta Content:  or\n",
      "---\n",
      "delta Content:  something\n",
      "---\n",
      "delta Content: .\n",
      "---\n",
      "delta Content:  Let\n",
      "---\n",
      "delta Content:  me\n",
      "---\n",
      "delta Content:  make\n",
      "---\n",
      "delta Content:  sure\n",
      "---\n",
      "delta Content:  I\n",
      "---\n",
      "delta Content:  don\n",
      "---\n",
      "delta Content: 't\n",
      "---\n",
      "delta Content:  add\n",
      "---\n",
      "delta Content:  unnecessary\n",
      "---\n",
      "delta Content:  words\n",
      "---\n",
      "delta Content: ,\n",
      "---\n",
      "delta Content:  but\n",
      "---\n",
      "delta Content:  keep\n",
      "---\n",
      "delta Content:  it\n",
      "---\n",
      "delta Content:  natural\n",
      "---\n",
      "delta Content: .\n",
      "---\n",
      "delta Content:  Let\n",
      "---\n",
      "delta Content:  me\n",
      "---\n",
      "delta Content:  put\n",
      "---\n",
      "delta Content:  it\n",
      "---\n",
      "delta Content:  all\n",
      "---\n",
      "delta Content:  together\n",
      "---\n",
      "delta Content: .\n",
      "\n",
      "---\n",
      "delta Content: </think>\n",
      "---\n",
      "delta Content: \n",
      "\n",
      "\n",
      "---\n",
      "delta Content: Hello\n",
      "---\n",
      "delta Content: !\n",
      "---\n",
      "delta Content:  ÔøΩ\n",
      "---\n",
      "delta Content:  Are\n",
      "---\n",
      "delta Content:  you\n",
      "---\n",
      "delta Content:  having\n",
      "---\n",
      "delta Content:  a\n",
      "---\n",
      "delta Content:  great\n",
      "---\n",
      "delta Content:  day\n",
      "---\n",
      "delta Content: ?\n",
      "---\n",
      "delta Content:  ÔøΩ\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mlx-community/Qwen3-0.6B-4bit\",\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(f\"delta Content: {chunk.choices[0].delta.content}\")\n",
    "    # print(chunk.choices[0].delta)\n",
    "    if hasattr(chunk.choices[0].delta, \"reasoning\"):\n",
    "        print(f\"delta Reasoning: {chunk.choices[0].delta.reasoning}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "42f673dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, the user greeted me with \"Hello!\" and my reply is \"Hello! Just so I can assist you today. How can I help you out?\" It looks like they're happy to chat and need some information.\n",
      "\n",
      "I should acknowledge their greeting and let them know I'm here to help. Maybe offer to print something, as that feels natural and friendly.\n",
      "\n",
      "I‚Äôll keep it simple and open-ended, inviting them to ask for more details.\n",
      "</think>\n",
      "\n",
      "Hello! Just so I can assist you today. How can I help you out?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mlx-community/DeepSeek-R1-Distill-Qwen-1.5B-4bit\",\n",
    "    messages=messages,\n",
    "    extra_body={\n",
    "        \"enable_thinking\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "content = completion.choices[0].message.content\n",
    "print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
