{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b169b3",
   "metadata": {},
   "source": [
    "# Audio Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db5819fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T09:11:10.434012Z",
     "start_time": "2025-05-14T09:11:10.234783Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Configure client to use local server\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:10240/v1\",  # Point to local server\n",
    "    api_key=\"not-needed\"  # API key is not required for local server\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bafa8f",
   "metadata": {},
   "source": "## TTS"
  },
  {
   "cell_type": "markdown",
   "id": "d1349a27",
   "metadata": {},
   "source": [
    "You can directly test using the curl method, as follows:\n",
    "\n",
    "```shell\n",
    "curl -X POST \"http://localhost:10240/v1/audio/speech\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer xxxx\" \\\n",
    "  -d '{\n",
    "    \"model\": \"lucasnewman/f5-tts-mlx\",\n",
    "    \"input\": \"MLX project is awsome.\",\n",
    "    \"voice\": \"alloy\"\n",
    "  }' \\\n",
    "  --output mlx.wav\n",
    "```\n",
    "\n",
    "You can also use OpenAI's Python SDK in the project for access, which can basically be done without feeling. As follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "280fe5e6-0c7f-4554-93a9-0d30cce21f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/07/bt1n4pzn5ln_b8ts86fztw9w0000gn/T/ipykernel_88777/4079694185.py:10: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
      "  response.stream_to_file(speech_file_path)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "speech_file_path = \"mlx_example.wav\"\n",
    "response = client.audio.speech.create(\n",
    "  model=\"lucasnewman/f5-tts-mlx\",\n",
    "  voice=\"alloy\", # voice si not working for now\n",
    "  input=\"MLX project is awsome.\",\n",
    ")\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[mlx-audio](https://github.com/Blaizzy/mlx-audio) The currently supported models can be viewed at [models](https://github.com/Blaizzy/mlx-audio/tree/main/mlx_audio/tts/models), below is a code example using the Kokoro model.",
   "id": "95d4f96a7e850e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fe98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "speech_file_path = \"mlx_example.wav\"\n",
    "response = client.audio.speech.create(\n",
    "  model=\"mlx-community/Kokoro-82M-4bit\",\n",
    "  voice=\"af_sky\", # voice si not working for now\n",
    "  input=\"MLX project is awsome.\",\n",
    ")\n",
    "response.stream_to_file(speech_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ede3909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mlx_example.wav:\n",
      "\n",
      " File Size: 133k      Bit Rate: 384k\n",
      "  Encoding: Signed PCM    \n",
      "  Channels: 1 @ 16-bit   \n",
      "Samplerate: 24000Hz      \n",
      "Replaygain: off         \n",
      "  Duration: 00:00:02.77  \n",
      "\n",
      "In:100%  00:00:02.77 [00:00:00.00] Out:133k  [      |      ] Hd:0.2 Clip:0    \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# play the generated audio file\n",
    "\n",
    "!play \"mlx_example.wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc803eca",
   "metadata": {},
   "source": [
    "## STT\n",
    "\n",
    "You can directly test using the curl method, as follows:\n",
    "\n",
    "```shell\n",
    "curl -X POST \"http://localhost:10240/v1/audio/transcriptions\" \\\n",
    "  -H \"Content-Type: multipart/form-data\" \\\n",
    "  -F \"file=@mlx_example.wav\" \\\n",
    "  -F \"model=mlx-community/whisper-large-v3-turbo\"\n",
    "```\n",
    "\n",
    "You can also use OpenAI's Python SDK in the project for access, which can basically be done without feeling. As follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "49b5e220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TranscriptionVerbose(duration=None, language='en', text=' MLX project is awesome!', segments=[TranscriptionSegment(id=0, avg_logprob=-0.7071904076470269, compression_ratio=0.7419354838709677, end=3.0, no_speech_prob=2.697437325588359e-12, seek=0, start=0.0, temperature=0.0, text=' MLX project is awesome!', tokens=[50365, 21601, 55, 1716, 307, 3476, 0, 50515])], words=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_file = open(\"mlx_example.wav\", \"rb\")\n",
    "transcript = client.audio.transcriptions.create(\n",
    "  model=\"mlx-community/whisper-large-v3-turbo\",\n",
    "  file=audio_file,\n",
    "  response_format=\"verbose_json\",\n",
    ")\n",
    "\n",
    "transcript"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
